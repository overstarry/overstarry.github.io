<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Kubernetes | Overstarry Site</title>
<meta name=keywords content><meta name=description content="overstarry site"><meta name=author content="overstarry"><link rel=canonical href=https://jasminides.com/tags/kubernetes/><meta name=google-site-verification content="gfdsdx"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://jasminides.com/img/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://jasminides.com/img/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://jasminides.com/img/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://jasminides.com/img/favicon/apple-touch-icon.png><link rel=mask-icon href=https://jasminides.com/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://jasminides.com/tags/kubernetes/index.xml><link rel=alternate hreflang=zh href=https://jasminides.com/tags/kubernetes/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-G40XG2SPQN"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-G40XG2SPQN")}</script><meta property="og:title" content="Kubernetes"><meta property="og:description" content="overstarry site"><meta property="og:type" content="website"><meta property="og:url" content="https://jasminides.com/tags/kubernetes/"><meta property="og:image" content="https://jasminides.com/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="overstarry site"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jasminides.com/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Kubernetes"><meta name=twitter:description content="overstarry site"><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2240998016636586" crossorigin=anonymous></script><link rel=manifest href=/site.webmanifest></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://jasminides.com/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://jasminides.com/archives/ title=archives><span>archives</span></a></li><li><a href=https://jasminides.com/tags/ title=tags><span>tags</span></a></li><li><a href=https://jasminides.com/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://jasminides.com/index.xml title=rss><span>rss</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://jasminides.com/>主页</a>&nbsp;»&nbsp;<a href=https://jasminides.com/tags/>Tags</a></div><h1>Kubernetes</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Kubernetes 系统资源预留</h2></header><div class=entry-content><p>前言 Kubernetes 的 pod 可以按照节点的资源进行调度，默认情况下 pod 能够使用节点的全部资源，这样往往会出现因为节点自身运行的一些驱动及 Kubernetes 系统守护进程，导致资源不足的问题。 例如有一个应用在运行中使用了大量的系统资源，导致 kubelet 和 apiserver 的心跳出现故障，导致节点处于 Not Ready 的状态，节点出现 Not Ready 的状况后，过一会儿会将 pod 调度到其它 node 节点上运行，往往会导致节点雪崩，一个接一个的出现 Not Ready 状况。
那么如何解决这个问题呢？这时可以通过 为 Kubernetes 集群配置资源预留，kubelet 暴露了一个名为 Node Allocatable 的特性，有助于为系统守护进程预留计算资源，Kubernetes 也是推荐集群管理员按照每个节点上的工作负载来配置 Node Allocatable。
Node Allocatable Kubernetes 节点上的 Allocatable 被定义为 Pod 可用计算资源量。调度器不会超额申请 Allocatable。目前支持 CPU、内存 和 存储 这几个参数。可以通过 kubectl describe node 命令查看节点可分配资源的数据： 可以看到有 Capacity 和 Allocatable 两个内容，Allocatable 这个就是节点可分配资源，由于没有设置，所以默认 Capacity 和 Allocatable 是一致的。
Capacity 是节点所有的系统资源，kube-reserved 是给 kube 组件预留的资源，system-reserved 是给系统进程预留的资源，eviction-hard 是 Kubelet 的驱逐阈值。
...</p></div><footer class=entry-footer><span title='2024-03-09 21:23:52 +0800 +0800'>三月 9, 2024</span>&nbsp;·&nbsp;overstarry</footer><a class=entry-link aria-label="post link to Kubernetes 系统资源预留" href=https://jasminides.com/posts/kubernetes_resource_reservation/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Kubernetes ExternalName</h2></header><div class=entry-content><p>前言 我们知道 kubernetes 内部服务之间是通过 service 进行相互访问的，那么如果现在有一个非 kubernetes 部署的服务，我们可以也通过 service 进行内部交互使用吗？答案是可以，我们可以使用 service 的 ExternalName 类型将 service 映射到外部服务上。
最近需要将一个外部服务映射到 kubernetes service 上，通过查找资料学习，本文记录如何将 kubernetes service 映射到外部服务的流程步骤。
外部域名映射内部 service 先讲解如何将外部服务通过域名的方式映射到内部 service 上，通过配置 externalName 字段来配置映射关系。例如，以下 Service 定义将 test 命名空间中的 my-service 服务映射到 my.overstarry.vip:
apiVersion: v1 kind: Service metadata: name: my-service namespace: test spec: type: ExternalName externalName: my.overstarry.vip 虽然 externalName 也支持填写 ip 地址，但不会被 kubernetes 解析，如果需要使用 ip 地址，可以使用无头服务 Headless，下文会进行介绍。
外部服务 ip 映射 service 接下来介绍没有域名的外部服务和 service 如何进行映射。上文讲过虽然 externalName 也支持填写 ip 地址，但不会被 kubernetes 解析，如果需要，则应该使用 Headless Service 进行映射。
...</p></div><footer class=entry-footer><span title='2024-02-24 13:52:30 +0800 +0800'>二月 24, 2024</span>&nbsp;·&nbsp;overstarry</footer><a class=entry-link aria-label="post link to Kubernetes ExternalName" href=https://jasminides.com/posts/kubernetes_externalname/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Kubernetes externaltrafficpolicy 简介</h2></header><div class=entry-content><p>前言 最近在使用 Kubernetes 查看 pod 日志时，发现 pod 日志显示的 ip 不是真实的请求者 ip, 而是 Node 节点的 ip。通过查阅资料发现可以通过设置 externalTrafficPolicy 来显示真实的 IP。
本文对 externaltrafficpolicy 进行一个简单的介绍。
简介 ExternalTrafficPolicy 是 Kubernetes Service 对象的一个属性，它决定了流量如何从集群外部访问 Service。有两个可选值：Cluster 和 Local。
Cluster 模式： 在 Cluster 模式下，流量将通过负载均衡器分发到 Service 的所有 Pod 上。这是传统的负载均衡方式，适用于需要水平扩展和容错的场景。负载均衡器会将流量平均分配给所有可用的 Pod，从而实现负载均衡。
Local 模式： 在 Local 模式下，流量将直接访问与请求最近的节点上运行的 Pod。这种方式避免了负载均衡器的介入，直接将流量定向到本地的 Pod 上。这样可以减少延迟，并且在负载均衡器发生故障时仍然保持可用性。
区别 两种模式有什么区别呢？
Cluster 模式 Cluster 模式是默认的模式，Kube-proxy 不管容器在哪个节点上，会公平的转发到某一个节点上，在转发时会替换掉源 ip，变成转发的上一个节点的 ip.原因是 Kube-proxy 在做转发的时候，会做一次 SNAT (source network address translation)，所以源 ip 变成了上一个节点的 ip 地址。
这个模式的优点是负载均衡比较好，缺点是由于转发，可能会有性能损耗。
Local 模式 Local 模式下，请求只转发给本机的容器，不会转发给其它节点的容器，保留了源 ip。
...</p></div><footer class=entry-footer><span title='2023-12-03 09:48:07 +0800 +0800'>十二月 3, 2023</span>&nbsp;·&nbsp;overstarry</footer><a class=entry-link aria-label="post link to Kubernetes externaltrafficpolicy 简介" href=https://jasminides.com/posts/kubernetes_externaltrafficpolicy/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Nginx Ingress http 请求 413 状态码问题及解决方法</h2></header><div class=entry-content><p>问题 最近在调用一个上传文件的接口时，发现接口调用响应状态码为 413，并且控制台显示跨域错误信息。查找了相关信息，得知 413 状态码表示请求的包体过大导致的。
出现这种情况，我想到了 2 种解决方案：1) 调整上传文件的方式 2) 调整网关的参数。
综合目前的现况，采取了第二种方式调整网关客户端请求体最大值的参数。
解决 通过查阅 nginx ingress 的文档，得知可以添加 nginx.ingress.kubernetes.io/proxy-body-size 注解来设置请求体的最大值，设置 nginx.ingress.kubernetes.io/proxy-body-size 值为合适的值后，再请求接口发现接口顺利响应。
小结 本文介绍了客户端请求接口时，由于 nginx 默认 proxy-body-size 参数太小，导致请求 413 的问题及相应的解决方案。
参考 https://opendocs.alipay.com/support/01rb44 https://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/annotations.md#custom-max-body-size</p></div><footer class=entry-footer><span title='2023-09-16 16:30:30 +0800 +0800'>九月 16, 2023</span>&nbsp;·&nbsp;overstarry</footer><a class=entry-link aria-label="post link to Nginx Ingress http 请求 413 状态码问题及解决方法" href=https://jasminides.com/posts/nginx-ingress_http%E8%AF%B7%E6%B1%82413%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Kubernetes Health check</h2></header><div class=entry-content><p>本文我来讲解 Kubernetes 中的一个重要概念：容器的健康检查。
介绍 在 Kubernetes 中，你可以为 Pod 里的容器定义一个健康检查“探针”（Probe）。 这样，kubelet 就会根据这个 Probe 的返回值决定这个容器的状态，而不是直接以容器镜像是否运行（来自 Docker 返回的信息）作为依据。 这种机制，是生产环境中保证应用健康存活的重要手段。
k8s 主要有三种健康检查的探针：1) LivenessProbe 存活探针 2) ReadinessProbe 就绪探针 3) StartupProbe 启动探针
kubelet 使用存活探针来确定什么时候要重启容器。例如，存活探针可以探测到应用死锁（应用程序在运行，但是无法继续执行后面的步骤）情况。重启这种状态下的容器有助于提高应用的可用性，即使其中存在缺陷。
存活探针的常见模式是为就绪探针使用相同的低成本 HTTP 端点，但具有更高的 failureThreshold。这样可以确保在硬性终止 Pod 之前，将观察到 Pod 在一段时间内处于非就绪状态。
kubelet 使用就绪探针可以知道容器何时准备好接受请求流量，当一个 Pod 内的所有容器都就绪时，才能认为该 Pod 就绪。这种信号的一个用途就是控制哪个 Pod 作为 Service 的后端。若 Pod 尚未就绪，会被从 Service 的负载均衡器中剔除。
kubelet 使用启动探针来了解应用容器何时启动。如果配置了这类探针，你就可以控制容器在启动成功后再进行存活性和就绪态检查，确保这些存活、就绪探针不会影响应用的启动。启动探针可以用于对慢启动容器进行存活性检测，避免它们在启动运行之前就被杀掉。
probe 介绍 接下来我来讲解用的较多的 2 个探针：1) LivenessProbe 存活探针 2) ReadinessProbe 就绪探针
LivenessProbe 许多应用由于长时间运行导致程序异常，需要重启服务才能继续正常使用，Kubernetes 提供了存活探针 (LivenessProbe) 来发现并处理这种情况。
我们先创建一个 pod, pod 的文件如下：
...</p></div><footer class=entry-footer><span title='2023-06-23 22:42:41 +0800 +0800'>六月 23, 2023</span>&nbsp;·&nbsp;overstarry</footer><a class=entry-link aria-label="post link to Kubernetes Health check" href=https://jasminides.com/posts/kubernetes-healthcheck/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Kubernetes pod 修改 hosts 文件</h2></header><div class=entry-content><p>前言 最近看了 k8s 的书，学习了一些新的知识，将会分几篇来介绍学习到的知识，本文来先介绍 k8s 中如何修改 pod 的 hosts 文件。
我们知道当 DNS 出现问题时，可以向 Pod 的/etc/hosts 文件添加条目来提供主机名解析 Pod 级别覆盖。该如何向 hosts 文件中添加条目呢？可以使用 PodSpec 中的 HostAliases 字段添加自定义条目。
虽然我们也可以直接进入 pod 修改 host 文件来实现，但这样 pod 重建时会被覆盖，所以我们应该使用 HostAliases 来进行修改，因为该文件会由 Kubelet 管理，并且 可以在 Pod 创建/重启过程中被重写。
使用 我们该如何操作呢，接下来由我来介绍使用步骤：
1 先创建 Deployment YAML 文件来创建后台运行的 busybox pod
apiVersion: apps/v1 kind: Deployment metadata: name: busybox-deployment spec: replicas: 1 selector: matchLabels: app: busybox template: metadata: labels: app: busybox spec: containers: - name: busybox image: busybox args: [ "sleep", "3600" ] resources: limits: memory: "128Mi" cpu: "500m" requests: memory: "64Mi" cpu: "250m" volumeMounts: - name: busybox-volume mountPath: /data volumes: - name: busybox-volume emptyDir: {} 查看 pod ip
...</p></div><footer class=entry-footer><span title='2023-06-03 21:37:05 +0800 +0800'>六月 3, 2023</span>&nbsp;·&nbsp;overstarry</footer><a class=entry-link aria-label="post link to Kubernetes pod 修改 hosts 文件" href=https://jasminides.com/posts/kubernetes_pod%E4%BF%AE%E6%94%B9hosts%E6%96%87%E4%BB%B6/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Helm 介绍及使用</h2></header><div class=entry-content><p>今天我来简单介绍 kubernetes 生态中一个重要一环 - 包管理工具 Helm。
介绍 Helm 是 Kubernetes 的开源包管理器。它提供了提供、共享和使用为 Kubernetes 构建的软件的能力。
Helm 于 2015 年在 Deis 创建，后来被微软收购。现在称为 Helm Classic 的是在当年 11 月的首届 KubeCon 上推出的。2016 年 1 月，Helm Classic 与谷歌的 Kubernetes 部署管理器合并到现在是 Helm 主要项目的存储库中。
该项目目前拥有超过 30,000 个 GitHub stars，每月从全球获得超过 200 万次下载。2020 年 4 月，Helm 在 CNCF 中获得毕业。
安装 Helm 二进制安装 1 打开 https://github.com/helm/helm/releases , 下载你需要的版本 2 解压安装包 3 将文件夹中的 helm 二进制文件移动到相应的位置
脚本安装 helm 官方提供了一个安装的脚本：
$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 $ chmod 700 get_helm.sh $ ./get_helm.sh 除了以上 2 种安装方式，你还可以通过各个操作系统的包管理工具安装和编译源码安装，这里就不过多赘述了。
...</p></div><footer class=entry-footer><span title='2023-02-04 14:31:08 +0800 +0800'>二月 4, 2023</span>&nbsp;·&nbsp;overstarry</footer><a class=entry-link aria-label="post link to Helm 介绍及使用" href=https://jasminides.com/posts/helm%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BD%BF%E7%94%A8/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Kubernetes Configmaps mounted with subPath not update when changed</h2></header><div class=entry-content><p>起因 最近在使用 k8s 部署应用时，我使用 ConfigMaps 的方式来挂载应用的配置文件。在我的知识储备中，k8s 修改 cm 的内容，pod 里的配置文件应该也会同步更新才是，但是我进入 pod , 发现配置还是旧版本没有更新，需要重启 pod 才会生效。
问题 那为什么配置没有及时更新呢？通过查阅资料，我发现使用 subPath 挂载的容器不会接收到配置更新。这是为什么呢，相比于没有使用 subPath 有什么区别呢？
subPath 使用了符号链接的方式挂载文件，容器内的文件是一个链接到存储在一个隐藏的带有时间戳目录中的同名文件。当 configMaps 更新时，符号链接会更新，但挂载在容器中的文件绑定保持不变。
解决 使用 path 字段为特定 ConfigMap 项指定所需的文件路径 具体如下：
apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: registry.k8s.io/busybox command: [ "/bin/sh","-c","cat /etc/config/keys" ] volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: special-config items: - key: SPECIAL_LEVEL path: keys restartPolicy: Never 亲测这样是可以正常更新的，但同目录下的其它文件会删除掉，看了几个相关的 issues , 发现你还可以手动创建符号链接到相应的文件夹，
小结 使用 subPath 挂载配置至容器时，配置更新时，容器内的配置不能同步更新，这是 k8s 官方处于各种原因做出的限制，目前还没有很好的办法来解决这个问题。
参考 https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#mounted-configmaps-are-updated-automatically https://github.com/kubernetes/kubernetes/issues/50345 https://github.com/kubernetes/kubernetes/blob/master/pkg/volume/util/atomic_writer.go</p></div><footer class=entry-footer><span title='2022-10-02 13:05:57 +0800 +0800'>十月 2, 2022</span>&nbsp;·&nbsp;overstarry</footer><a class=entry-link aria-label="post link to Kubernetes Configmaps mounted with subPath not update when changed" href=https://jasminides.com/posts/kubernetes-configmaps-subpath-no-reload/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Prometheus_operato</h2></header><div class=entry-content><p>安装 Metrics Server 有了 Metrics Server，用户就可以访问 Kubernetes 核心监控数据（core metrics）。这其中包括了 Pod、Node、容器、Service 等主要 Kubernetes 核心概念的 Metrics。
Resource MetricsAPI: https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/resource-metrics-api.md kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml 部署 Prometheus kube-prometheus 下载存储库 git clone https://github.com/prometheus-operator/kube-prometheus 使用 manifests 中的配置文件创建监控 stack cd kube-prometheus kubectl create -f manifests/setup until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo ""; done kubectl create -f manifests/ 访问 dashboards 通过 kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090 就能展现prometheus ui grafana
kubectl --namespace monitoring port-forward svc/grafana 3000 默认账户密码 admin/admin，进入后会要求修改密码，可以看到已经有了预添加了数据源 可以看到有了许多 K8S 监控的默认看板 ...</p></div><footer class=entry-footer><span title='2022-08-23 22:18:08 +0800 +0800'>八月 23, 2022</span>&nbsp;·&nbsp;overstarry</footer><a class=entry-link aria-label="post link to Prometheus_operato" href=https://jasminides.com/posts/prometheus_operato/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>K8s_Finalizers</h2></header><div class=entry-content><p>起因 在我们日常使用 k8s 中，可能会遇到这样的情况：在删除 namespace 时，往往会遇到资源没有被删除的情况，资源处于 terminating 的状态，这时我们该如何解决了，寻找到的解决方法往往是如下：
1 运行以下命令查看处于 terminating 状态的资源 (这里以 namespace 为例):
kubectl get namespaces
2 选择一个 Terminating namespace，并查看 namespace 中的 finalizer。运行以下命令：
kubectl get namespace &lt;terminating-namespace> -o yaml
得到类似这样的信息：
apiVersion: v1 kind: Namespace metadata: creationTimestamp: "2021-01-20T15:18:06Z" deletionTimestamp: "2021-01-21T02:50:02Z" name: &lt;terminating-namespace> resourceVersion: "3249493" selfLink: /api/v1/namespaces/knative-eventing uid: f300ea38-c8c2-4653-b432-b66103e412db spec: finalizers: - kubernetes status: phase: Terminating 3 导出 json 格式到 tmp.json:
...</p></div><footer class=entry-footer><span title='2022-01-23 11:21:44 +0800 +0800'>一月 23, 2022</span>&nbsp;·&nbsp;overstarry</footer><a class=entry-link aria-label="post link to K8s_Finalizers" href=https://jasminides.com/posts/k8s_finalizers/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://jasminides.com/tags/kubernetes/page/2/>下一页&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>Copyright © 2024 - overstarry · All rights reserved<br></span>·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>